import os
import json
import time
import random
import argparse
from pathlib import Path
from datetime import datetime
import arxiv
from playwright.sync_api import Download, sync_playwright, TimeoutError as PWTimeout
import pyperclip

# ===== é…ç½® =====
PAPERS_DIR = "papers"
SUMMARIES_DIR = "summaries"
HISTORY_FILE = "history.json"
AUTH_STATE_FILE = "auth_state.json"

SEARCH_QUERY = "causal"
MAX_RESULTS = 2
PROMPT_TEMPLATE = "è¯·æ€»ç»“è¿™ç¯‡è®ºæ–‡çš„é¢˜ç›®ï¼Œä½œè€…æœºæ„ï¼Œè´¡çŒ®ä¸æ–¹æ³•ï¼Œä¸ç”¨å…¬å¼è¡¨è¾¾ã€‚åªéœ€è¦å›ç­”ä¸éœ€è¦å…¶ä»–å†…å®¹ã€‚"
COPILOT_URL = "https://copilot.microsoft.com"


os.makedirs(PAPERS_DIR, exist_ok=True)
os.makedirs(SUMMARIES_DIR, exist_ok=True)

# ===== å†å²è®°å½• =====
def load_history():
    return set(json.load(open(HISTORY_FILE, encoding="utf-8"))) if os.path.exists(HISTORY_FILE) else set()

def save_history(history):
    json.dump(list(history), open(HISTORY_FILE, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

# ===== æŠ“å–æ–°è®ºæ–‡ =====

def fetch_new_papers(history, max_results=MAX_RESULTS,
                     base_delay: float = 1.0,
                     jitter: float = 0.5,
                     max_delay: float = 8.0):
    """
    æŠ“å– arXiv æ–°è®ºæ–‡ï¼Œå¸¦æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨é˜²é™æµã€‚
    :param history: set / History å¯¹è±¡ï¼Œè®°å½•å·²å¤„ç†çš„è®ºæ–‡ ID
    :param max_results: æœ¬æ¬¡æœ€å¤šæŠ“å¤šå°‘ç¯‡
    :param base_delay: åˆå§‹é—´éš”ï¼ˆç§’ï¼‰
    :param jitter: éšæœºæŠ–åŠ¨èŒƒå›´ Â±jitter
    :param max_delay: æœ€å¤§é—´éš”ï¼ˆç§’ï¼‰
    :return: list[Path] æ–°ä¸‹è½½çš„ PDF æ–‡ä»¶è·¯å¾„
    """
    search = arxiv.Search(
        query=SEARCH_QUERY,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    client = arxiv.Client()
    new_files = []

    delay = base_delay
    for result in client.results(search):
        pid = result.get_short_id()
        if pid in history:
            continue

        pdf_path = Path(PAPERS_DIR) / f"{pid}.pdf"
        result.download_pdf(filename=str(pdf_path))
        new_files.append(pdf_path)
        history.add(pid)
        print(f"[â¬‡] ä¸‹è½½: {pid}")

        # ---- é˜²é™æµç­‰å¾… ----
        time.sleep(delay + random.uniform(-jitter, jitter))
        delay = min(delay * 2, max_delay)  # æŒ‡æ•°é€€é¿ï¼Œå°é¡¶ max_delay

    return new_files

# ===== ç™»å½•ç¡®è®¤ =====
def wait_for_login_confirm():
    input("\nğŸ’¡ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•å¹¶è¿›å…¥ Copilot å¯¹è¯ç•Œé¢ï¼Œç„¶åå›è½¦ç»§ç»­...")

# ===== åˆå§‹åŒ–ç™»å½•æ¨¡å¼ =====
def init_auth_state():
    with sync_playwright() as p:
        browser = p.chromium.launch(channel="chrome", headless=False)
        context = browser.new_context()
        page = context.new_page()
        print("[â–¶] æ‰“å¼€ Copilotï¼Œè¯·æ‰‹åŠ¨ç™»å½•...")
        page.goto(COPILOT_URL, wait_until="domcontentloaded", timeout=120000)
        for text in ["æ¥å—æ‰€æœ‰", "åŒæ„", "Accept all", "Agree"]:
            try:
                btn = page.get_by_text(text, exact=False)
                if btn and btn.count() > 0:
                    btn.first.click(timeout=2000)
                    print(f"[ğŸª] ç‚¹å‡»äº† Cookie æŒ‰é’®: {text}")
                    break
            except:
                pass
        wait_for_login_confirm()
        context.storage_state(path=AUTH_STATE_FILE)
        print(f"[âœ…] ç™»å½•çŠ¶æ€å·²ä¿å­˜åˆ° {AUTH_STATE_FILE}")
        browser.close()

# ===== æ‰“å¼€å·²ç™»å½•ç¯å¢ƒ =====
def open_copilot_with_auth(p):
    if not os.path.exists(AUTH_STATE_FILE):
        raise FileNotFoundError("æœªæ‰¾åˆ°ç™»å½•ä¼šè¯ï¼Œè¯·å…ˆè¿è¡Œ --init-auth å®Œæˆä¸€æ¬¡ç™»å½•ã€‚")
    browser = p.chromium.launch(channel="chrome", headless=False)
    context = browser.new_context(storage_state=AUTH_STATE_FILE)
    page = context.new_page()
    page.goto(COPILOT_URL, wait_until="domcontentloaded", timeout=120000)
    return browser, context, page

# ===== ä»å®¹å™¨æå–æ–‡æœ¬ =====
def extract_answer_text(page):
    sel = 'div.space-y-3.mt-3'
    try:
        page.wait_for_selector(sel, timeout=120000)
        texts = page.locator(sel).all_inner_texts()
        return "\n".join(t.strip() for t in texts if t.strip())
    except Exception as e:
        print(f"[âš ] æå–å›ç­”å¤±è´¥: {e}")
        return ""

# ===== æ‘˜è¦ç”Ÿæˆ =====
def summarize_one_pdf(page, pdf_path):
    print(f"[ğŸ“‚] ä¸Šä¼ æ–‡ä»¶: {Path(pdf_path).name}")
    page.set_input_files('input[type="file"]', pdf_path)
    try:
        page.wait_for_selector('div[class*="file-card"], div:has-text("å·²ä¸Šä¼ ")', timeout=30000)
        print("[âœ…] æ–‡ä»¶ä¸Šä¼ å®Œæˆ")
    except PWTimeout:
        print("[âš ] ä¸Šä¼ å®Œæˆæç¤ºæœªæ£€æµ‹åˆ°ï¼Œç»§ç»­")

    page.fill('textarea', PROMPT_TEMPLATE)
    page.keyboard.press("Enter")
    print("[âœ‰] å·²å‘é€æ‘˜è¦è¯·æ±‚")

    # ç­‰å¤åˆ¶æŒ‰é’®å‡ºç°
    copy_btn_sel = '[data-testid="copy-message-button"]'
    try:
        page.wait_for_selector(copy_btn_sel, timeout=300000)
        print("[ğŸ“‹] æ£€æµ‹åˆ°å¤åˆ¶æŒ‰é’®ï¼Œå¼€å§‹ç›‘å¬å®¹å™¨æ–‡æœ¬...")
    except PWTimeout:
        raise RuntimeError("ç­‰å¾…å¤åˆ¶æŒ‰é’®è¶…æ—¶")

    # ç›‘å¬å˜åŒ–
    start_time = time.time()
    last_text = ""
    last_change = time.time()
    idle_time = 12
    min_output_time = 15
    max_wait = 480

    while True:
        current_text = extract_answer_text(page)
        if current_text and current_text != last_text:
            last_text = current_text
            last_change = time.time()
        now = time.time()
        if (now - start_time) >= min_output_time and (now - last_change) >= idle_time:
            break
        if (now - start_time) >= max_wait:
            print("[âš ] è¾¾åˆ°æœ€å¤§ç­‰å¾…æ—¶é—´ï¼Œç»“æŸç›‘å¬")
            break
        time.sleep(1)

    # å¤åˆ¶æŒ‰é’®å…œåº•ï¼ˆè·å–åŒ…å«å…¬å¼çš„å†…å®¹ï¼‰
    text_final = last_text
    try:
        page.click(copy_btn_sel, timeout=3000)
        time.sleep(0.5)
        copied_text = copied_text = pyperclip.paste()
        if copied_text and copied_text.strip():
            print("[âœ…] ä»å¤åˆ¶æŒ‰é’®è·å–åˆ°å®Œæ•´å†…å®¹ï¼ˆå«å…¬å¼ï¼‰")
            text_final = copied_text
    except Exception as e:
        print(f"[âš ] å¤åˆ¶æŒ‰é’®è·å–å¤±è´¥: {e}")

    if not text_final.strip():
        raise RuntimeError("æœªè·å–åˆ°ä»»ä½•å›ç­”å†…å®¹")

    
    return text_final

# ===== ä¸»æµç¨‹ï¼šæ¯ç¯‡å•ç‹¬æµè§ˆå™¨ =====
def run_pipeline():
    history = load_history()
    new_files = fetch_new_papers(history)
    if not new_files:
        print("[â„¹] æ²¡æœ‰æ–°è®ºæ–‡")
        save_history(history)
        return
    for pdf_path in new_files:
        fid = Path(pdf_path).stem
        try:
            with sync_playwright() as p:
                browser, context, page = open_copilot_with_auth(p)
                summary = summarize_one_pdf(page, pdf_path)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_path = os.path.join(SUMMARIES_DIR, f"{fid}_{timestamp}.md")
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(summary)
                print(f"[ğŸ’¾] æ‘˜è¦å·²ä¿å­˜ (mdæ ¼å¼): {save_path}")
                browser.close()
        except Exception as e:
            print(f"[âŒ] {fid} å¤„ç†å¤±è´¥: {e}")
            try:
                shot_path = os.path.join(SUMMARIES_DIR, f"{fid}_error.png")
                page.screenshot(path=shot_path, full_page=True)
                print(f"[ğŸ“¸] é”™è¯¯æˆªå›¾å·²ä¿å­˜: {shot_path}")
            except:
                pass
        save_history(history)

# ===== å…¥å£ =====
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="arXivâ†’Copilot è‡ªåŠ¨åŒ–")
    parser.add_argument("--init-auth", action="store_true", help="åˆå§‹åŒ–ç™»å½•å¹¶ä¿å­˜ä¼šè¯")
    args = parser.parse_args()
    if args.init_auth:
        init_auth_state()
    else:
        run_pipeline()
